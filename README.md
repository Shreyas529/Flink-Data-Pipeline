# Flink Data Pipeline

A real-time streaming Click-Through Rate (CTR) pipeline built with Apache Flink and Apache Kafka. This project demonstrates a complete end-to-end streaming data architecture for processing ad events and computing CTR metrics in real-time.

## Overview

This pipeline simulates ad event data generation, streams events through Kafka, processes them using Apache Flink's Table API, and calculates latency metrics for performance analysis.

### Architecture

```
┌─────────────┐     ┌───────────┐     ┌───────────────┐     ┌─────────────┐
│  Producers  │────▶│   Kafka   │────▶│     Flink     │────▶│   Results   │
│  (Events)   │     │  (events) │     │  (Processor)  │     │   (Kafka)   │
└─────────────┘     └───────────┘     └───────────────┘     └─────────────┘
```

> **Note**: The pipeline also supports Apache Spark as an alternative processor via the `--impl spark` option.
                                                                   │
                                                                   ▼
                                                            ┌─────────────┐
                                                            │   Latency   │
                                                            │  Calculator │
                                                            └─────────────┘
```

## Features

- **Multi-producer event generation** with configurable throughput
- **MMPP (Markov-Modulated Poisson Process)** for realistic bursty traffic simulation
- **Apache Flink Table API** for stream processing with tumbling windows
- **CTR computation** per campaign within 10-second windows
- **End-to-end latency tracking** with CSV report generation
- **PostgreSQL integration** for storing ad-campaign mappings
- **Support for both Flink and Spark** processing engines

## Prerequisites

- Python 3.8+
- Apache Kafka (running on `localhost:9092`)
- Apache Flink (with PyFlink)
- PostgreSQL (optional, for database operations)

### Python Dependencies

```bash
pip install kafka-python pyflink numpy
```

## Configuration

All configuration options are defined in `config.py`:

| Parameter | Default | Description |
|-----------|---------|-------------|
| `BOOTSTRAP_SERVERS` | `localhost:9092` | Kafka bootstrap servers |
| `NUM_PRODUCERS` | 3 | Number of parallel producers |
| `NUM_CONSUMERS` | 3 | Number of parallel consumers |
| `EVENTS_PER_PRODUCER` | 5000 | Total events generated per producer |
| `DURATION_SECONDS` | 60 | Total duration of event generation |
| `NUM_CAMPAIGNS` | 10 | Number of advertising campaigns |
| `ADS_PER_CAMPAIGN` | 100 | Number of ads per campaign |
| `PARTITIONS` | 3 | Kafka topic partitions |
| `DISTRIBUTION` | `mmpp` | Event distribution (`normal` or `mmpp`) |

### Database Configuration

```python
DB_HOST = "localhost"
DB_PORT = 5432
DB_NAME = "sds"
DB_USER = "myuser"
DB_PASS = "mypassword"
```

## Usage

### Running the Complete Pipeline

```bash
python driver.py --impl flink
```

#### Command Line Options

| Option | Description |
|--------|-------------|
| `--impl {spark,flink,none}` | Choose the streaming implementation (default: flink) |
| `--skip-consumers` | Skip starting consumer processes |
| `--num-producers N` | Override number of producers |
| `--events-per-producer N` | Override events per producer |
| `--duration N` | Override duration in seconds |

### Running Individual Components

**Start producers only:**
```bash
python producer.py
```

**Run Flink processor (Table API):**
```bash
python flink_processor_table_api.py
```

**Run latency calculator:**
```bash
python latency_calculator.py
```

**Clear Kafka topics:**
```bash
python clear_topics.py
```

## Project Structure

```
├── config.py                     # Configuration parameters
├── driver.py                     # Main orchestrator script
├── producer.py                   # Kafka event producers
├── data_generation.py            # Synthetic event generation
├── mmpp.py                       # Markov-Modulated Poisson Process generator
├── flink_processor_table_api.py  # Flink Table API processing
├── latency_calculator.py         # End-to-end latency measurement
├── clear_topics.py               # Kafka topic cleanup utility
└── README.md                     # This file
```

## Event Schema

Events generated by producers follow this structure:

```json
{
  "user_id": "uuid",
  "page_id": "uuid",
  "ad_id": "uuid",
  "campaign_id": 1,
  "ad_type": "banner|video|popup|native|sponsored|interstitial|rewarded",
  "event_type": "view|click|purchase",
  "event_time_ns": 1234567890000000000,
  "window_id": 0,
  "ip_address": "192.168.1.1"
}
```

## Output

### CTR Results (Kafka Topic: `results`)

```json
{
  "window_id": 1,
  "window_start": "2024-01-01 00:00:00",
  "campaign_id": 5,
  "ctr": 0.15,
  "max_source_log_append_time_ms": 1234567890000
}
```

### Latency Report

After pipeline completion, a CSV file is generated:

```
latency_report_<producers>_<total_events>_<distribution>.csv
```

Contains:
- `window_id`: Processing window identifier
- `max_latency_ms`: Maximum end-to-end latency in milliseconds

## MMPP Traffic Model

The pipeline uses a Markov-Modulated Poisson Process to simulate realistic bursty traffic patterns:

- **Two states**: Low rate and high rate (configurable ratio)
- **State transitions**: Governed by continuous-time Markov chain
- **Average throughput**: Maintained at configured `EVENTS_PER_PRODUCER`

## License

This project is available for educational and research purposes.
